\documentclass{article}
\usepackage[pdftex]{hyperref}
\usepackage[pdftex]{graphicx}
\usepackage{fullpage}
\usepackage{color}
\usepackage{hyperref}
\usepackage{wrapfig}
\title{ORC File Format Specification}
\author{Owen O'Malley}
\date{October 2014}
\begin{document}
\maketitle

\section{Introduction}

\begin{wrapfigure}{r}{3.1in}
  \centering
  \includegraphics[width=3in]{ORCFileStructure.pdf}
  \caption{ORC file top level structure}
  \label{orc-structure}
\end{wrapfigure}

Hive's RCFile was the standard format for storing tabular data in
Hadoop for several years. However, RCFile has limitations because it
treats each column as a binary blob without semantics. In Hive 0.11 we
added a new file format named Optimized Row Columnar (ORC) file that
uses and retains the type information from the table definition. ORC
uses type specific readers and writers that provide light weight
compression techniques such as dictionary encoding, bit packing, delta
encoding, and run length encoding -- resulting in dramatically smaller
files. Additionally, ORC can apply generic compression using zlib, or
Snappy on top of the lightweight compression for even smaller
files. However, storage savings are only part of the gain. ORC
supports projection, which selects subsets of the columns for reading,
so that queries reading only one column read only the required
bytes. Furthermore, ORC files include light weight indexes that
include the minimum and maximum values for each column in each set of
10,000 rows and the entire file. Using pushdown filters from Hive, the
file reader can skip entire sets of rows that aren't important for
this query.

\section{File Tail}

Since HDFS does not support changing the data in a file after it is
written, ORC stores the top level index at the end of the file. The
overall structure of the file is given in figure~\ref{orc-structure}.
The file's tail consists of 3 parts- the file metadata, file footer,
and postscript. 

The metadata for ORC is stored using
\href{http://s.apache.org/protobuf_encoding}{Protocol Buffers}, which
provides the ability to add new fields without breaking readers. This
document incorporates the Protobuf definition from the
\href{http://s.apache.org/orc_proto}{ORC source code} and the reader
is encouraged to review the Protobuf encoding if they need to understand
the byte-level encoding

\subsection{Postscript}

The Postscript section provides the necessary information to interpret
the rest of the file including the length of the file's Footer and
Metadata sections, the version of the file, and the kind of general
compression used (eg. none, zlib, or snappy). The Postscript is never
compressed and ends one byte before the end of the file.  The version
stored in the Postscript is the lowest version of Hive that is
guaranteed to be able to read the file and it stored as a sequence of
the major and minor version. There are currently two versions that are
used: [0,11] for Hive 0.11, and [0,12] for Hive 0.12 to 0.14.

The process of reading an ORC file works backwards through the
file. Rather than making multiple short reads, the ORC reader reads
the last 16k bytes of the file with the hope that it will contain both
the Footer and Postscript sections. The final byte of the file
contains the serialized length of the Postscript, which must be less
than 256 bytes. Once the Postscript is parsed, the compressed
serialized length of the Footer is known and it can be decompressed
and parsed.

\begin{verbatim}
message PostScript {
  // the length of the footer section in bytes
  optional uint64 footerLength = 1;
  // the kind of generic compression used
  optional CompressionKind compression = 2;
  // the maximum size of each compression chunk
  optional uint64 compressionBlockSize = 3;
  // the version of the writer
  repeated uint32 version = 4 [packed = true];
  // the length of the metadata section in bytes
  optional uint64 metadataLength = 5;
  // the fixed string "ORC"
  optional string magic = 8000;
}

enum CompressionKind {
  NONE = 0;
  ZLIB = 1;
  SNAPPY = 2;
  LZO = 3;
}
\end{verbatim}

\subsection{Footer}

The Footer section contains the layout of the body of the file, the
type schema information, the number of rows, and the statistics about
each of the columns. 

The file is broken in to three parts- Header, Body, and Tail. The
Header consists of the bytes ``ORC'' to support tools that want to
scan the front of the file to determine the type of the file. The Body
contains the rows and indexes, and the Tail gives the file level
information as described in this section.

\begin{verbatim}
message Footer {
  // the length of the file header in bytes (always 3)
  optional uint64 headerLength = 1;
  // the length of the file body in bytes
  optional uint64 contentLength = 2;
  // the information about the stripes
  repeated StripeInformation stripes = 3;
  // the schema information
  repeated Type types = 4;
  // the user metadata that was added
  repeated UserMetadataItem metadata = 5;
  // the total number of rows in the file
  optional uint64 numberOfRows = 6;
  // the statistics of each column across the file
  repeated ColumnStatistics statistics = 7;
  // the maximum number of rows in each index entry
  optional uint32 rowIndexStride = 8;
}
\end{verbatim}

\subsubsection{Stripe Information}

The body of the file is divided into stripes. Each stripe is self
contained and may be read using only its own bytes combined with the
file's Footer and Postscript. Each stripe contains only entire rows so
that rows never straddle stripe boundaries. Stripes have three
sections: a set of indexes for the rows within the stripe, the data
itself, and a stripe footer. Both the indexes and the data sections
are divided by columns so that only the data for the required columns
needs to be read.

\begin{verbatim}
message StripeInformation {
  // the start of the stripe within the file
  optional uint64 offset = 1;
  // the length of the indexes in bytes
  optional uint64 indexLength = 2;
  // the length of the data in bytes
  optional uint64 dataLength = 3;
  // the length of the footer in bytes
  optional uint64 footerLength = 4;
  // the number of rows in the stripe
  optional uint64 numberOfRows = 5;
}
\end{verbatim}

\subsubsection{Type Information}

\begin{wrapfigure}{r}{2.5in}
  \centering
  \includegraphics[width=2.4in]{TreeWriters.pdf}
  \caption{Type Tree}
  \label{type-tree}
  \vspace{-20pt}
\end{wrapfigure}

All of the rows in an ORC file must have the same schema.  Logically
the schema is expressed as a tree as in figure~\ref{type-tree}, where
the compound types have subcolumns under them. 

The equivalent Hive DDL for figure~\ref{type-tree} would be:
\begin{verbatim}
create table Foobar (
  myInt int,
  myMap map<string, 
            struct<myString : string, 
                   myDouble: double>>,
  myTime timestamp
);
\end{verbatim}

The type tree is flattened in to a list via a pre-order traversal
where each type is assigned the next id. Clearly the root of the type
tree is always type id 0. Compound types have a field named subtypes
that contains the list of their children's type ids.

\begin{verbatim}
message Type {
  enum Kind {
    BOOLEAN = 0;
    BYTE = 1;
    SHORT = 2;
    INT = 3;
    LONG = 4;
    FLOAT = 5;
    DOUBLE = 6;
    STRING = 7;
    BINARY = 8;
    TIMESTAMP = 9;
    LIST = 10;
    MAP = 11;
    STRUCT = 12;
    UNION = 13;
    DECIMAL = 14;
    DATE = 15;
    VARCHAR = 16;
    CHAR = 17;
  }
  // the kind of this type
  required Kind kind = 1;
  // the type ids of any subcolumns for list, map, struct, or union
  repeated uint32 subtypes = 2 [packed=true];
  // the list of field names for struct
  repeated string fieldNames = 3;
  // the maximum length of the type for varchar or char
  optional uint32 maximumLength = 4;
  // the precision and scale for decimal
  optional uint32 precision = 5;
  optional uint32 scale = 6;
}
\end{verbatim}

\subsubsection{Column Statistics}

The goal of the column statistics is that for each column, the writer
records the count and depending on the type other useful fields.  For
most of the primitive types, it records the minimum and maximum
values; and for numeric types it additionally stores the sum.

\begin{verbatim}
message ColumnStatistics {
  // the number of values
  optional uint64 numberOfValues = 1;

  // At most one of these has a value for any column
  optional IntegerStatistics intStatistics = 2;
  optional DoubleStatistics doubleStatistics = 3;
  optional StringStatistics stringStatistics = 4;
  optional BucketStatistics bucketStatistics = 5;
  optional DecimalStatistics decimalStatistics = 6;
  optional DateStatistics dateStatistics = 7;
  optional BinaryStatistics binaryStatistics = 8;
  optional TimestampStatistics timestampStatistics = 9;
}
\end{verbatim}

For integer types (tinyint, smallint, int, bigint), the column
statistics includes the minimum, maximum, and sum. If the sum
overflows long at any point during the calculation, no sum is
recorded.

\begin{verbatim}
message IntegerStatistics  {
  optional sint64 minimum = 1;
  optional sint64 maximum = 2;
  optional sint64 sum = 3;
}
\end{verbatim}

For floating point types (float, double), the column statistics
include the minimum, maximum, and sum. If the sum overflows a double,
no sum is recorded.

\begin{verbatim}
message DoubleStatistics {
  optional double minimum = 1;
  optional double maximum = 2;
  optional double sum = 3;
}
\end{verbatim}

For strings, the minimum value, maximum value, and the sum of the
lengths of the values are recorded.

\begin{verbatim}
message StringStatistics {
  optional string minimum = 1;
  optional string maximum = 2;
  // sum will store the total length of all strings
  optional sint64 sum = 3;
}
\end{verbatim}

For booleans, the statistics include the count of false and true values.

\begin{verbatim}
message BucketStatistics {
  repeated uint64 count = 1 [packed=true];
}
\end{verbatim}

For decimals, the minimum, maximum, and sum are stored.

\begin{verbatim}
message DecimalStatistics {
  optional string minimum = 1;
  optional string maximum = 2;
  optional string sum = 3;
}
\end{verbatim}

Date columns record the minimum and maximum values as the number of days since
the epoch (1/1/2015).

\begin{verbatim}
message DateStatistics {
  // min,max values saved as days since epoch
  optional sint32 minimum = 1;
  optional sint32 maximum = 2;
}
\end{verbatim}

Timestamp columns record the minimum and maximum values as the number of 
milliseconds since the epoch (1/1/2015).

\begin{verbatim}
message TimestampStatistics {
  // min,max values saved as milliseconds since epoch
  optional sint64 minimum = 1;
  optional sint64 maximum = 2;
}
\end{verbatim}

For binary columns, the sum of the value lengths is stored.

\begin{verbatim}
message BinaryStatistics {
  // sum will store the total binary blob length
  optional sint64 sum = 1;
}
\end{verbatim}

\subsubsection{User Metadata}

The user can add arbitrary key/value pairs to an ORC file as it is
written. The contents of the keys and values are completely
application defined, but the key is a string and the value is
binary. Care should be taken by applications to make sure that their
keys are unique and in general should be prefixed with an organization
code.

\begin{verbatim}
message UserMetadataItem {
  // the user defined key
  required string name = 1;
  // the user defined binary value
  required bytes value = 2;
}
\end{verbatim}

\subsection{File Metadata}

The file Metadata section contains column statistics at the stripe
level granularity. These statistics enable input split elimination
based on the predicate push-down evaluated per a stripe.

\begin{verbatim}
message StripeStatistics {
  repeated ColumnStatistics colStats = 1;
}

message Metadata {
  repeated StripeStatistics stripeStats = 1;
}
\end{verbatim}

\section{Compression Streams}

\begin{wrapfigure}{r}{2.5in}
  \centering
  \includegraphics[width=2.4in]{CompressionStream.pdf}
  \caption{Compression Stream Structure}
  \label{compression-stream}
  \vspace{-20pt}
\end{wrapfigure}

If the ORC file writer selects a generic compression codec (zlib or
snappy), every part of the ORC file except for the Postscript is
compressed with that codec.  However, one of the requirements for ORC
is that the reader be able to skip over compressed bytes without
decompressing the entire stream. To manage this, ORC writes compressed
streams in chunks with headers as in figure~\ref{compression-stream}.
To handle uncompressable data, if the compressed data is larger than
the original, the original is stored. Each header is 3 bytes long with
$ compressedLength * 2 + isOriginal $ stored as a big endian
value. Each compression chunk is compressed independently so that as
long as a decompressor starts at the top of a header, it can start
decompressing without the previous bytes.

The default compression chunk size is 256K, but writers can choose
their own value.  The chunk size is always recorded in the Postscript
so that readers can allocate appropriately sized buffers.

For files that do not use generic compression, each stream is stored
directly with no headers.

\section{Stripes}

The body of ORC files consists of a series of stripes. Stripes are
large (typically ~200MB) and independent of each other and are often
processed by different tasks. The defining characteristic for columnar
storage formats is that the data for each column is stored separately
and that reading data out of the file should be proportional to the
number of columns read.

In ORC files, each column is stored separately and composed of several
streams. For example, an integer column is represented as two streams
PRESENT, which uses one with a bit per value recording if the value is
non-null, and DATA, which records the non-null values. If all of a
column's values in a stripe are non-null, the PRESENT stream is
omitted.

\subsection{Stripe Footer}

The stripe footer contains the encoding of each column and the
directory of the streams including their location.

\begin{verbatim}
message StripeFooter {
  // the location of each stream
  repeated Stream streams = 1;
  // the encoding of each column
  repeated ColumnEncoding columns = 2;
}
\end{verbatim}


\begin{verbatim}
message Stream {
  enum Kind {
    PRESENT = 0;
    DATA = 1;
    LENGTH = 2;
    DICTIONARY_DATA = 3;
    DICTIONARY_COUNT = 4;
    SECONDARY = 5;
    ROW_INDEX = 6;
  }
  required Kind kind = 1;
  optional uint32 column = 2;
  optional uint64 length = 3;
}
\end{verbatim}

Columns may be encoded using either a direct encoding or a dictionary
encoding. In Hive 0.12, we added RLEv2 and thus added DIRECT\_V2 and
DICTIONARY\_V2

\begin{verbatim}
message ColumnEncoding {
  enum Kind {
    DIRECT = 0;
    DICTIONARY = 1;
    DIRECT_V2 = 2;
    DICTIONARY_V2 = 3;
  }
  required Kind kind = 1;
  optional uint32 dictionarySize = 2;
}
\end{verbatim}

\subsection{Column Encodings}

\subsubsection{Integer Columns}

\subsubsection{Floating Point Columns}

\subsubsection{String Columns}

\subsubsection{Boolean Columns}
\subsubsection{Binary Columns}

\subsubsection{Decimal Columns}

\subsubsection{Date Columns}

\subsubsection{Timestamp Columns}
\subsubsection{Char and Varchar Columns}

\subsubsection{Struct Columns}
\subsubsection{List Columns}
\subsubsection{Map Columns}
\subsubsection{Union Columns}

\subsection{Indexes}

\end{document}
